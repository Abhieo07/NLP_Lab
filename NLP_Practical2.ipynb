{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gautam Mandal\n",
        "\n",
        " CMPN-A 2102A0063\n",
        "\n",
        " https://github.com/Abhieo07/NLP_Lab/blob/main/NLP_Practical2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNjzwB63b-iX",
        "outputId": "ff3c3e11-1214-45f2-892b-54ea15f617be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| Word          | Lemma         | Porter Stem   | Lancaster Stem   | Snowball Stem   |\n",
            "+===============+===============+===============+==================+=================+\n",
            "| Natural       | Natural       | natur         | nat              | natur           |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| Language      | Language      | languag       | langu            | languag         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| Processing    | Processing    | process       | process          | process         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| or            | or            | or            | or               | or              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| Computational | Computational | comput        | comput           | comput          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| Linguistics   | Linguistics   | linguist      | lingu            | linguist        |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| (             | (             | (             | (                | (               |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| NLP/CL        | NLP/CL        | nlp/cl        | nlp/cl           | nlp/cl          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| )             | )             | )             | )                | )               |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| deals         | deal          | deal          | deal             | deal            |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| with          | with          | with          | with             | with            |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| understanding | understanding | understand    | understand       | understand      |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| and           | and           | and           | and              | and             |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| developing    | developing    | develop       | develop          | develop         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| computational | computational | comput        | comput           | comput          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| theories      | theory        | theori        | the              | theori          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| of            | of            | of            | of               | of              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| human         | human         | human         | hum              | human           |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| language      | language      | languag       | langu            | languag         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| .             | .             | .             | .                | .               |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| Such          | Such          | such          | such             | such            |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| theories      | theory        | theori        | the              | theori          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| allow         | allow         | allow         | allow            | allow           |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| us            | u             | us            | us               | us              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| to            | to            | to            | to               | to              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| understand    | understand    | understand    | understand       | understand      |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| the           | the           | the           | the              | the             |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| structure     | structure     | structur      | structure        | structur        |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| of            | of            | of            | of               | of              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| language      | language      | languag       | langu            | languag         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| and           | and           | and           | and              | and             |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| build         | build         | build         | build            | build           |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| computer      | computer      | comput        | comput           | comput          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| software      | software      | softwar       | softw            | softwar         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| that          | that          | that          | that             | that            |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| can           | can           | can           | can              | can             |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| process       | process       | process       | process          | process         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| language      | language      | languag       | langu            | languag         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| .             | .             | .             | .                | .               |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| NLP/CL        | NLP/CL        | nlp/cl        | nlp/cl           | nlp/cl          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| is            | is            | is            | is               | is              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| expected      | expected      | expect        | expect           | expect          |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| to            | to            | to            | to               | to              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| play          | play          | play          | play             | play            |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| a             | a             | a             | a                | a               |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| major         | major         | major         | maj              | major           |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| role          | role          | role          | rol              | role            |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| in            | in            | in            | in               | in              |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| facilitating  | facilitating  | facilit       | facilit          | facilit         |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n",
            "| man-machine   | man-machine   | man-machin    | man-machine      | man-machin      |\n",
            "+---------------+---------------+---------------+------------------+-----------------+\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Download the 'punkt' tokenizer and 'wordnet' lemmatizer resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Initialize LancasterStemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "# Initialize SnowballStemmer (use English language)\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Sample text\n",
        "text = (\"Natural Language Processing or Computational Linguistics (NLP/CL) deals with \"\n",
        "        \"understanding and developing computational theories of human language. Such theories \"\n",
        "        \"allow us to understand the structure of language and build computer software that can \"\n",
        "        \"process language. NLP/CL is expected to play a major role in facilitating man-machine \")\n",
        "\n",
        "# Tokenize the text\n",
        "tokenization = nltk.word_tokenize(text)\n",
        "\n",
        "# Prepare data for tabular display\n",
        "table_data = []\n",
        "for w in tokenization:\n",
        "    lemmatized = wordnet_lemmatizer.lemmatize(w)\n",
        "    porter_stemmed = porter_stemmer.stem(w)\n",
        "    lancaster_stemmed = lancaster_stemmer.stem(w)\n",
        "    snowball_stemmed = snowball_stemmer.stem(w)\n",
        "\n",
        "    table_data.append([w, lemmatized, porter_stemmed, lancaster_stemmed, snowball_stemmed])\n",
        "\n",
        "# Define table headers\n",
        "headers = [\"Word\", \"Lemma\", \"Porter Stem\", \"Lancaster Stem\", \"Snowball Stem\"]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn3gIlrRfObn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
